{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/home\n"
     ]
    }
   ],
   "source": [
    "# checking current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "for root, dirs, files in os.walk(filepath):              # to create a list of files and collect each filepath\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))   # joining the file path and roots with the subdirectories using glob\n",
    "#     print(file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8056\n"
     ]
    }
   ],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "\n",
    "    \n",
    "# creating a csv reader object to read the csv files, for every filepath in the file path list, \n",
    "# then extracting each data row one by one and appending it\n",
    "for f in file_path_list:\n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        for line in csvreader:\n",
    "            full_data_rows_list.append(line) \n",
    "\n",
    "            \n",
    "# printing the total number of rows at the end of the process\n",
    "print(len(full_data_rows_list))\n",
    "\n",
    "\n",
    "# creating a smaller event data csv file to insert data into the Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# number of rows written on the csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Apache Cassandra Session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster and the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS udacity \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 1 - to get the artist name, song title, and song's lengh in a particular session and item, we need to create the table with session ID and itemInSession as the primary key that would make a more efficient query due to the unique indentification of the record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a table with these columns - sessionId, itemInSession, artist, lenght, and song. The primary key would be composed of sessionId as the partition key and itemInSession as the clustering column because they are required to filter the rows. The other 3 columns are required in the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS event_data_new \"\n",
    "query = query + \"(sessionId int, itemInSession int, artist text,  length double, song text, \\\n",
    "                    PRIMARY KEY (sessionId, itemInSession))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an Appache Cassandra table from the CSV file, we need to read the data from CSV and assign them to appropriate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)              # skip header\n",
    "    \n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO event_data_new (sessionId, itemInSession, artist, length, song)\"\n",
    "        query = query + \"VALUES (%s, %s,%s, %s, %s)\"\n",
    "        \n",
    "        try:\n",
    "            session.execute(query, (\n",
    "                int(line[8]),  # sessionId (convert to integer)\n",
    "                int(line[3]),  # itemInSession (convert to integer)\n",
    "                line[0],  # artist\n",
    "                float(line[5]),  # length (convert to float)\n",
    "                line[9],  # song\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query should use the event_data_new model to fetch the data required. To do that, we need to select sessionId, itemInSession, artist, length, song columns from the event_data_new table and filter the table by sessionId and itemInSession column. Since the 2 columns are already set as the primary key, it should easily fetch the required rows to answer the questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithless Music Matters (Mark Knight Dub) 495.3073\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT sessionId, itemInSession, artist, length, song \\\n",
    "        FROM event_data_new \\\n",
    "         WHERE sessionId = 338 AND itemInSession = 4\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.artist, row.song, row.length,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 2: Fetch only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\\\n",
    "for userid = 10, sessionid = 182\n",
    "\n",
    "The resulting table will have userId, sessionId, itemInSession, song, artist, firstName, and lastName columns. The composite primary key would be composed of userId and sessionId to group data by user and session and itemInSession as the clustering column. This will enable us to sort data by itemInSession as well as identifying rows by user and session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create a table named artist_song_user where userId (to uniquely identify each user and display their first and last names) and sessionId (to identify the session the user was in) would be set as the composite primary key and the clustering column would be itemInSession because we need to sort the resulting table by this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS artist_song_user \"\n",
    "query = query + \"(userId int, sessionId int, itemInSession int, song text, artist text, \\\n",
    "                    firstName text, lastName text, \\\n",
    "                    PRIMARY KEY ((userId, sessionId), itemInSession))\\\n",
    "                    WITH CLUSTERING ORDER BY (itemInSession ASC);\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)              # skip header\n",
    "    \n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO artist_song_user (userId, sessionId, itemInSession, song, artist, firstName, lastName)\"\n",
    "        query = query + \"VALUES (%s, %s,%s, %s, %s, %s, %s)\"\n",
    "        \n",
    "        try:\n",
    "            session.execute(query, (\n",
    "                int(line[10]),  # userId (convert to integer)\n",
    "                int(line[8]),  # sessionId (convert to integer)\n",
    "                int(line[3]),  # itemInSession (convert to integer)\n",
    "                line[9],  # song\n",
    "                line[0],  # artist\n",
    "                line[1],  # firstName\n",
    "                line[4],  # lastName\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the userId, sessionId, itemInSession, song, artist, firstName, lastName from the artist_song_user table to answer the questions. The table was modeled using userId and sessionId as the primary key to better run the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Down To The Bone Keep On Keepin' On Sylvie Cruz\n",
      "1 Three Drives Greece 2000 Sylvie Cruz\n",
      "2 Sebastien Tellier Kilometer Sylvie Cruz\n",
      "3 Lonnie Gordon Catch You Baby (Steve Pitron & Max Sanna Radio Edit) Sylvie Cruz\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT userId, sessionId, itemInSession, song, artist, firstName, lastName\\\n",
    "            FROM artist_song_user \\\n",
    "            WHERE userId = 10 AND sessionId = 182\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.iteminsession, row.artist, row.song, row.firstname, row.lastname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 3: every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "As the filter needs to be set on the song and the results need to be presented by the user, we need a table with song, userId, firstName, and lastName columns and set the primary key of the table based on song and userId. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS song_user \"\n",
    "query = query + \"(song text, userId int, firstName text, lastName text, \\\n",
    "                    PRIMARY KEY (song, userId))\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)              # skip header\n",
    "    \n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO song_user (song, userId, firstName, lastName)\"\n",
    "        query = query + \"VALUES (%s, %s,%s, %s)\"\n",
    "        \n",
    "        try:\n",
    "            session.execute(query, (\n",
    "                line[9],  # song\n",
    "                int(line[10]),  # userId (convert to integer)\n",
    "                line[1],  # firstName\n",
    "                line[4],  # lastName\n",
    "            ))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting song, userId, firstName, lastName columns from song_user table to display the first and last name of the users who listed to a particular song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacqueline Lynch\n",
      "Tegan Levine\n",
      "Sara Johnson\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT song, userId, firstName, lastName \\\n",
    "         FROM song_user \\\n",
    "         WHERE song = 'All Hands Against His Own'\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "#     print(row._fields)\n",
    "    print (row.firstname, row.lastname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping the tables created before closing the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"DROP table event_data_new\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"DROP table artist_song_user\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"DROP table song_user\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
